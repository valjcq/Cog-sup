{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bz6OR80MkEO"
      },
      "source": [
        "Copy this notebook and don't forget to **ADD YOUR NAME** in the name of the copy.\n",
        "\n",
        "Answers to send by email by next Wednesday evening:\n",
        "\n",
        "\n",
        "*   subject : **ML1 LAB4**\n",
        "*   (and not ML1-LAB4 or ML1LAB4) etc...\n",
        "*   with the link to your colab, with edit rights (cf. sharing settings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TqQYwcl1lLa"
      },
      "source": [
        "## TODO1 : linear prediction in numpy\n",
        "\n",
        "Suppose we have already learnt a multiclass classifier into 3 classes, with matrix weight W and bias vector b.\n",
        "\n",
        "(below we just set them randomly, but suppose they are the result of a learning phase)\n",
        "\n",
        "Suppose we have 4 input objects to classify (matrix X below).\n",
        "\n",
        "Implement how to predict the class\n",
        "- for the full batch X\n",
        "- for a single row in X (take the first row)\n",
        "\n",
        "Tip: look for the numpy argmax method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "W0V-A-1k16bY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predictions: [1 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# weight matrix and bias vector\n",
        "W = np.random.rand(10,3)  # one column of weights per class\n",
        "b = np.random.rand(3)     # one bias value per class\n",
        "\n",
        "# 4 input vectors of size 10\n",
        "X = np.random.rand(4,10)\n",
        "\n",
        "# Predict the class for the full batch X\n",
        "distance = np.dot(X, W) + b\n",
        "predictions = np.argmax(distance, axis=1)\n",
        "print(\"predictions:\", predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2ZMrNoJ-XVE"
      },
      "source": [
        "# The scikit-learn python framework\n",
        "\n",
        "[Scikit-learn](https://scikit-learn.org/stable/) is a machine learning python library, which implements various regression, classification and clustering algorithms.\n",
        "\n",
        "It's imported as `sklearn`.\n",
        "\n",
        "Note sklearn is much used for linear and log-linear models, and less for deep learning.\n",
        "\n",
        "The classification and regression parts on the home page both point to the same general page on supervised learning: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rBRVbj6pooB"
      },
      "source": [
        "#Scikit-learn Vectorizers to get BOW vectors\n",
        "\n",
        "In last lab we coded how to transform a collection of documents into matrices of \"**bag of words**\" representations of these documents (the X_train and X_test matrices of LAB2 and 3).\n",
        "\n",
        "Scikit-learn has \"vectorizer\" methods to do that :\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "These matrices use the scipy.sparse type, which is appropriate for **sparse matrices**.\n",
        "\n",
        "All the vectorizers modules have 3 methods:\n",
        "- **fit** : builds the vocabulary and the correspondance between word forms and word ids\n",
        "- **transform** : transforms the documents into matrices of counts\n",
        "- **fit_transform** : performs both actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HFon2MNWpooE"
      },
      "outputs": [],
      "source": [
        "#\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# a French corpus (to see what is going on with diacritics)\n",
        "train_corpus = [\n",
        "     'Ceci est un document.',\n",
        "     \"Aujourd'hui, ce document est à moi.\",\n",
        "     'Et voilà le troisième.',\n",
        "     'Le premier document est-il le plus intéressant?',\n",
        " ]\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# the vectorizer is empty : this generates an error\n",
        "#print(vectorizer.vocabulary_)\n",
        "#print(vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnt0M563qibk"
      },
      "source": [
        "## The fit_transform method\n",
        "\n",
        "Calling `fit_transform` on train_corpus will :\n",
        "- tokenize the text : it will split it into \"words\" using a regular expression to define what can be a separator between words\n",
        "  - NB: this is a very uninformed and rough tokenization, meaning the obtained tokens are not always words as defined in linguistics\n",
        "- identify the vocabulary and associate an id to each element of the vocabulary\n",
        "- AND transform the training set into a matrix of BOW vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-zzi_0pqi3b",
        "outputId": "10bba78b-81c5-4479-8d71-ec071f522584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type of X_train <class 'scipy.sparse._csr.csr_matrix'>\n",
            "shape of X_train (4, 16)\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 21 stored elements and shape (4, 16)>\n",
            "  Coords\tValues\n",
            "  (0, 2)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 14)\t1\n",
            "  (0, 3)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 10)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 15)\t1\n",
            "  (2, 9)\t1\n",
            "  (2, 13)\t1\n",
            "  (3, 4)\t1\n",
            "  (3, 3)\t1\n",
            "  (3, 9)\t2\n",
            "  (3, 12)\t1\n",
            "  (3, 7)\t1\n",
            "  (3, 11)\t1\n",
            "  (3, 8)\t1\n",
            "[[0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1]\n",
            " [0 0 0 1 1 0 0 1 1 2 0 1 1 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X_train = vectorizer.fit_transform(train_corpus)\n",
        "\n",
        "# the matrix is sparse\n",
        "print(\"type of X_train\", type(X_train))\n",
        "print(\"shape of X_train\", X_train.shape)\n",
        "print(X_train)\n",
        "\n",
        "# here it is as a standard matrix\n",
        "print(X_train.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoMhMBlrpooF",
        "outputId": "3f2071fd-1812-4dc5-ea86-245453d56f78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ceci': 2, 'est': 4, 'un': 14, 'document': 3, 'aujourd': 0, 'hui': 6, 'ce': 1, 'moi': 10, 'et': 5, 'voilà': 15, 'le': 9, 'troisième': 13, 'premier': 12, 'il': 7, 'plus': 11, 'intéressant': 8}\n",
            "['aujourd' 'ce' 'ceci' 'document' 'est' 'et' 'hui' 'il' 'intéressant' 'le'\n",
            " 'moi' 'plus' 'premier' 'troisième' 'un' 'voilà']\n"
          ]
        }
      ],
      "source": [
        "# here is the mapping between word forms and ids (our \"w2i\" in previous lab session)\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# the list of word forms (our i2w)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmBe5rrrq6Ce"
      },
      "source": [
        "## TODO2 : answer the following comprehension questions:\n",
        "- What is the size of the vocabulary\n",
        "\n",
        "The vocabulary size is 16.\n",
        "\n",
        "- What does the 4th column of X.train.toarray() represent ?\n",
        "\n",
        "The 4th column of X_train.toarray() represents the number of times the 4th word (\"est\" in this case) appears in each document.\n",
        "\n",
        "- What is printed when printing the sparse matrix ?\n",
        "\n",
        "This not print a real matrix, but the structure of the matrix, with the non-zero elements and their position in the matrix. It's a better way to store the matrix, because it's more efficient in terms of memory and computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgz6MPONrHbz"
      },
      "source": [
        "## The transform method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiYn80OdpooF",
        "outputId": "55bc5ae1-4133-4ad3-f2c3-f49da2b825e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ceci': 2, 'est': 4, 'un': 14, 'document': 3, 'aujourd': 0, 'hui': 6, 'ce': 1, 'moi': 10, 'et': 5, 'voilà': 15, 'le': 9, 'troisième': 13, 'premier': 12, 'il': 7, 'plus': 11, 'intéressant': 8}\n",
            "shape of X_test (2, 16)\n"
          ]
        }
      ],
      "source": [
        "test_corpus = [ 'Ah un nouveau document.',\n",
        "              'Et ceci est encore un document.']\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_corpus)\n",
        "X_test = vectorizer.transform(test_corpus)\n",
        "print(\"shape of X_test\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyjGb3turVBl"
      },
      "source": [
        "## TODO3: analysis\n",
        "- What happened to the words in test_corpus that are not present in train_corpus?\n",
        "\n",
        "They are just ignored, because the vectorizer has been trained on the train_corpus, so it doesn't know the words from the test_corpus.\n",
        "- Compare to vectorizer.fit_transform\n",
        "\n",
        "The vectorizer.fit_transform method create the vocabulary and the correspondance between word forms and word ids, and transform the documents into matrices of counts. The transform method only transform the documents into matrices of counts, using the vocabulary and the correspondance between word forms and word ids created by the fit method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JJNB5UGrmcj"
      },
      "source": [
        "## TODO4: changing the parameters\n",
        "\n",
        "We are now providing input sentences in which tokens have all been separated by spaces.\n",
        "\n",
        "1.   How can you change the tokenization that the CountVectorizer will use ? (see its constructor)\n",
        "\n",
        "The tokenization can be changed within the token_pattern parameter, which is a regular expression that defines what is considered as a token. But to use this parameter, the analyzer parameter must be set to 'word'.\n",
        "\n",
        "2.   in particular, how can you make CountVectorizer split on spaces only?\n",
        "\n",
        "The token_pattern parameter can be set to r\"(?u)[\\w\\-\\']+\", which will split on spaces only.\n",
        "\n",
        "Indications: study https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html to see all the members of the instance, and deduce which member to modify:\n",
        "\n",
        "\n",
        "1.   Find out which parameters to modify to switch to bigram and trigram of **characters** features, and print the obtained vocabulary\n",
        "    - this means that the vocabulary will not be made of words, but of sequences of characters, of length 2 (character bigram) or 3 (character trigram)\n",
        "\n",
        "To have only two characters in one token, the analyzer parameter must be set to 'char' and the ngram_range parameter must be set to (2, 2). To have three characters in one token, the ngram_range parameter must be set to (3, 3).\n",
        "\n",
        "4.   Study the TfidfVectorizer class\n",
        " https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html and deduce how to easily obtain TF.IDF weigthed vector representations of the documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDQRFDtZpooG",
        "outputId": "30941016-f7e5-4bf8-ab26-3ddad590e018"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['-il', \"aujourd'hui\", 'ce', 'ceci', 'document', 'encore', 'est',\n",
              "       'et', 'intéressant', 'le', 'moi', 'plus', 'premier', 'troisième',\n",
              "       'un', 'voilà', 'à'], dtype=object)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_corpus = [\n",
        "     'Ceci est un document .',\n",
        "     \"Aujourd'hui , ce document est encore un document à moi .\",\n",
        "     'Et voilà le troisième .',\n",
        "     'Le premier document est -il le plus intéressant ?',\n",
        " ]\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.set_params(analyzer='word')\n",
        "vectorizer.set_params(token_pattern=r\"(?u)[\\w\\-\\']+\")\n",
        "X_train = vectorizer.fit_transform(train_corpus)\n",
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "C3WArRrrpooG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.         0.         0.64065543 0.40892206 0.         0.40892206\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.5051001  0.        ]\n"
          ]
        }
      ],
      "source": [
        "train_corpus = [\n",
        "     'Ceci est un document .',\n",
        "     \"Aujourd'hui , ce document est encore un document à moi .\",\n",
        "     'Et voilà le troisième .',\n",
        "     'Le premier document est -il le plus intéressant ?',\n",
        " ]\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "Tfvectorizer = TfidfVectorizer()\n",
        "X_train = Tfvectorizer.fit_transform(train_corpus)\n",
        "print(X_train.toarray()[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
